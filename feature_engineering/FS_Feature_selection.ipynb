{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe94b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import TargetEncoder, OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn import set_config\n",
    "set_config(enable_metadata_routing=True)\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from time import monotonic\n",
    "from prince import FAMD\n",
    "\n",
    "from utils.data_processing import load_data, raw_columns, full_dtypes, transform_datetime, df_ua_parser, transform_ipinfo, transform_packetinfo, transform_proxyinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2181af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "if data_path.joinpath(\"first_ml_processing.csv\").exists():\n",
    "    processed_data = pd.read_csv(data_path.joinpath(\"first_ml_processing.csv\"))\n",
    "else:\n",
    "    # Must use clean_data function to load data \n",
    "    input_data_path = Path(\"./data/cybersecurity_attacks.csv\")\n",
    "    dtypes = {col: col_type for col, col_type in full_dtypes.items() if col in raw_columns}\n",
    "    raw_data = load_data(input_data_path, dtype=dtypes)\n",
    "\n",
    "    datetime_columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"IsWeekend\"]\n",
    "    raw_data[datetime_columns] = transform_datetime(raw_data[\"Timestamp\"])\n",
    "    device_columns = [\"String\",\"Browser Name\", \"Browser Version\", \"Browser Minor\", \"Browser Patch\",\n",
    "                    \"Browser Patch Minor\", \"OS Name\", \"OS Version\", \"OS Version Minor\",\n",
    "                    \"OS Version Patch\", \"OS Version Patch Minor\", \"Device Brand\", \"Device Model\",\n",
    "                    \"Device Type\"]\n",
    "    raw_data[device_columns] = df_ua_parser(raw_data[\"Device Information\"])\n",
    "    proxy_columns = [\"Is Proxy\"]\n",
    "    raw_data[proxy_columns] = transform_proxyinfo(raw_data[\"Proxy Information\"])\n",
    "    ip_columns = [\"Int Source IP\", \"Int Destination IP\", \"Global Source IP\", \"Global Destination IP\"]\n",
    "    raw_data[ip_columns] = transform_ipinfo(raw_data[[\"Source IP Address\", \"Destination IP Address\"]])\n",
    "    packet_columns = [\"Packet Bin\"]\n",
    "    raw_data[packet_columns] = transform_packetinfo(raw_data[\"Packet Length\"], scale=False)\n",
    "\n",
    "    processed_data = raw_data.drop(columns=[\"Payload Data\",\"Timestamp\", \"String\", \"Device Information\",\n",
    "                                    \"Proxy Information\", \"Source IP Address\", \"Destination IP Address\"])\n",
    "    processed_data.to_csv(data_path.joinpath(\"first_ml_processing.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2655c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = processed_data[\"Attack Type\"].copy()\n",
    "X_dataset = processed_data.copy().drop(columns=[\"Attack Type\", \"Browser Patch\" , \"Browser Patch Minor\",\n",
    "                                                \"OS Version\", \"OS Version Minor\", \"OS Version Patch\", \"OS Version Patch Minor\",\n",
    "                                                \"Device Type\", \"User Information\", \"Geo-location Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a5017",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77724017",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = X_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "bool_cols = X_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "passthrough_columns = [col for col in X_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "        ])\n",
    "boolean_transformer = Pipeline([\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"bool\", boolean_transformer, bool_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"pca__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"pca\", PCA(n_components=6, random_state=124)),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n",
    "\n",
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the grid search with raw data? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=2 , pre_dispatch=\"n_jobs\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d01108",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = X_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "bool_cols = X_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "passthrough_columns = [col for col in X_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "boolean_transformer = Pipeline([\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"bool\", boolean_transformer, bool_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"pca__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"pca\", PCA(n_components=6, random_state=124)),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n",
    "\n",
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the grid search with an ordinal encoder for categorical data? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=2, pre_dispatch=\"n_jobs\")\n",
    "    with parallel_backend('threading', n_jobs=2):\n",
    "        gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fce166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat_dataset = X_dataset.copy()\n",
    "columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"Browser Version\", \"Browser Minor\"]\n",
    "Xcat_dataset[columns] = Xcat_dataset[columns].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda76794",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xcat_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xcat_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "bool_cols = Xcat_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "passthrough_columns = [col for col in Xcat_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        ])\n",
    "boolean_transformer = Pipeline([\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"bool\", boolean_transformer, bool_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"pca__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"pca\", PCA(n_components=6, random_state=124)),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n",
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the PCA analysis with more categorical columns? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xcat_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=5, error_score=\"raise\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490004f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat_dataset = X_dataset.copy()\n",
    "columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"Browser Version\", \"Browser Minor\"]\n",
    "Xcat_dataset[columns] = Xcat_dataset[columns].astype(\"str\")\n",
    "bool_cols = Xcat_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xcat_dataset[bool_cols] = Xcat_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8fa82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SimpleImputer.__init__() got an unexpected keyword argument 'sparse_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m passthrough_columns = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m Xcat_dataset.columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_cols \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m bool_cols \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m num_cols]\n\u001b[32m      5\u001b[39m numeric_transformer = Pipeline(\n\u001b[32m      6\u001b[39m         steps = [\n\u001b[32m      7\u001b[39m             (\u001b[33m\"\u001b[39m\u001b[33mimputer\u001b[39m\u001b[33m\"\u001b[39m, SimpleImputer(strategy=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m      8\u001b[39m             (\u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m, StandardScaler())\n\u001b[32m      9\u001b[39m         ])\n\u001b[32m     11\u001b[39m cat_transformer = Pipeline([\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mimputer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mSimpleImputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissing_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconstant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m),\n\u001b[32m     13\u001b[39m         ])\n\u001b[32m     15\u001b[39m preprocessor = ColumnTransformer(\n\u001b[32m     16\u001b[39m         transformers=[\n\u001b[32m     17\u001b[39m             (\u001b[33m\"\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m, cat_transformer, cat_cols),\n\u001b[32m     18\u001b[39m             (\u001b[33m\"\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m\"\u001b[39m, numeric_transformer, num_cols)\n\u001b[32m     19\u001b[39m         ]).set_output(transform=\u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m param_grid = {\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclassifier__n_estimators\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m],\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclassifier__max_depth\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclassifier__min_samples_split\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m2\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m30\u001b[39m],\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfamd__n_components\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m2\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m15\u001b[39m]\n\u001b[32m     26\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: SimpleImputer.__init__() got an unexpected keyword argument 'sparse_output'"
     ]
    }
   ],
   "source": [
    "cat_cols = Xcat_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xcat_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "passthrough_columns = [col for col in Xcat_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ]).set_output(transform=\"pandas\")\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"famd__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"famd\", FAMD()),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a45e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/usr/lib/python3.12/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 360 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n360 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1336, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 613, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 547, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/joblib/memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/utils.py\", line 28, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/pca.py\", line 251, in fit_transform\n    self.fit(X)\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/utils.py\", line 28, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/famd.py\", line 61, in fit\n    self.cat_scaler_.transform(X_cat),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py\", line 1035, in transform\n    raise ValueError(\nValueError: Pandas output does not support sparse data. Set sparse_output=False to output pandas dataframes or disable Pandas output via` ohe.set_output(transform=\"default\").\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m start_time = monotonic()\n\u001b[32m      7\u001b[39m gs = GridSearchCV(pipeline, param_grid, cv=\u001b[32m5\u001b[39m, n_jobs=\u001b[32m2\u001b[39m, pre_dispatch=\u001b[33m\"\u001b[39m\u001b[33mn_jobs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTime taken to fit the model: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m % (monotonic() - start_time))\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel score: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[33m\"\u001b[39m % gs.score(X_test, y_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1053\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1047\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1048\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1049\u001b[39m     )\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1057\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1612\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1611\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1030\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1027\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m   1028\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:479\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    473\u001b[39m     all_fits_failed_message = (\n\u001b[32m    474\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    478\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    482\u001b[39m     some_fits_failed_message = (\n\u001b[32m    483\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    489\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 360 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n360 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1336, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 613, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 547, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/joblib/memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/utils.py\", line 28, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/pca.py\", line 251, in fit_transform\n    self.fit(X)\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/utils.py\", line 28, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/prince/famd.py\", line 61, in fit\n    self.cat_scaler_.transform(X_cat),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py\", line 1035, in transform\n    raise ValueError(\nValueError: Pandas output does not support sparse data. Set sparse_output=False to output pandas dataframes or disable Pandas output via` ohe.set_output(transform=\"default\").\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the FAMD analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xcat_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=2, pre_dispatch=\"n_jobs\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_processed = preprocessor.fit_transform(Xcat_dataset, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat_dataset = X_dataset.copy()\n",
    "columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"Browser Version\", \"Browser Minor\"]\n",
    "Xcat_dataset[columns] = Xcat_dataset[columns].astype(\"str\")\n",
    "bool_cols = Xcat_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xcat_dataset[bool_cols] = Xcat_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9463bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xcat_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xcat_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "passthrough_columns = [col for col in Xcat_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 335.10 seconds\n",
      "Model score: 0.334\n",
      "Best parameters:  {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the HalvingGridSearchCV analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xcat_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = HalvingGridSearchCV(pipeline, param_grid, resource = \"classifier__n_estimators\", min_resources=10 , max_resources=500)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451cc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Int Source IP\", \"Int Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\", \"Packet Type\", \"Traffic Type\", \"Attack Signature\"]\n",
    "Xsim_dataset = X_dataset[columns].copy()\n",
    "bool_cols = Xsim_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xsim_dataset[bool_cols] = Xsim_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463e6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xsim_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xsim_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "passthrough_columns = [col for col in Xsim_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0255be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 97.31 seconds\n",
      "Model score: 0.335\n",
      "Best parameters:  {'classifier__max_depth': 20, 'classifier__min_samples_split': 30, 'classifier__n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the simple HalvingGridSearchCV analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xsim_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = HalvingGridSearchCV(pipeline, param_grid, resource = \"classifier__n_estimators\", min_resources=10 , max_resources=1000, factor=2)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8712388",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Source Port\", \"Destination Port\", \"Protocol\", \"Packet Type\", \"Traffic Type\", \"Attack Signature\", \"Network Segment\"]\n",
    "Xsim_dataset = X_dataset[columns].copy()\n",
    "bool_cols = Xsim_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xsim_dataset[bool_cols] = Xsim_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1243357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xsim_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xsim_dataset.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "passthrough_columns = [col for col in Xsim_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc08f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 35.26 seconds\n",
      "Model score: 0.333\n",
      "Best parameters:  {'classifier__max_depth': 10, 'classifier__min_samples_split': 30, 'classifier__n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the simple HalvingGridSearchCV analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xsim_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = HalvingGridSearchCV(pipeline, param_grid, resource = \"classifier__n_estimators\", min_resources=10 , max_resources=500)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217ab8c",
   "metadata": {},
   "source": [
    "## Recursive feature elimination with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42dbff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
