{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe94b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import TargetEncoder, OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import set_config\n",
    "set_config(enable_metadata_routing=True)\n",
    "from imblearn.combine import SMOTETomek\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from time import monotonic\n",
    "from prince import FAMD, MCA\n",
    "\n",
    "\n",
    "from utils.data_processing import load_data, raw_columns, full_dtypes, transform_datetime, df_ua_parser, transform_ipinfo, transform_packetinfo, transform_proxyinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2181af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "if not data_path.joinpath(\"first_ml_processing.csv\").exists():\n",
    "    # Must use clean_data function to load data \n",
    "    input_data_path = Path(\"./data/cybersecurity_attacks.csv\")\n",
    "    dtypes = {col: col_type for col, col_type in full_dtypes.items() if col in raw_columns}\n",
    "    raw_data = load_data(input_data_path, dtype=dtypes)\n",
    "\n",
    "    datetime_columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"IsWeekend\"]\n",
    "    raw_data[datetime_columns] = transform_datetime(raw_data[\"Timestamp\"])\n",
    "    device_columns = [\"String\",\"Browser Name\", \"Browser Version\", \"Browser Minor\", \"Browser Patch\",\n",
    "                    \"Browser Patch Minor\", \"OS Name\", \"OS Version\", \"OS Version Minor\",\n",
    "                    \"OS Version Patch\", \"OS Version Patch Minor\", \"Device Brand\", \"Device Model\",\n",
    "                    \"Device Type\"]\n",
    "    raw_data[device_columns] = df_ua_parser(raw_data[\"Device Information\"])\n",
    "    proxy_columns = [\"Is Proxy\"]\n",
    "    raw_data[proxy_columns] = transform_proxyinfo(raw_data[\"Proxy Information\"])\n",
    "    ip_columns = [\"Int Source IP\", \"Int Destination IP\", \"Global Source IP\", \"Global Destination IP\"]\n",
    "    raw_data[ip_columns] = transform_ipinfo(raw_data[[\"Source IP Address\", \"Destination IP Address\"]])\n",
    "    packet_columns = [\"Packet Bin\"]\n",
    "    raw_data[packet_columns] = transform_packetinfo(raw_data[\"Packet Length\"], scale=False)\n",
    "\n",
    "    processed_data = raw_data.drop(columns=[\"Payload Data\",\"Timestamp\", \"String\", \"Device Information\",\n",
    "                                    \"Proxy Information\", \"Source IP Address\", \"Destination IP Address\"])\n",
    "    processed_data.to_csv(data_path.joinpath(\"first_ml_processing.csv\"), index=False)\n",
    "processed_data = pd.read_csv(data_path.joinpath(\"first_ml_processing.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2655c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = processed_data[\"Attack Type\"].copy()\n",
    "X_dataset = processed_data.copy().drop(columns=[\"Attack Type\", \"Browser Patch\" , \"Browser Patch Minor\",\n",
    "                                                \"OS Version\", \"OS Version Minor\", \"OS Version Patch\", \"OS Version Patch Minor\",\n",
    "                                                \"Device Type\", \"User Information\", \"Geo-location Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a5017",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d01108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 648.73 seconds\n",
      "Model score: 0.335\n",
      "Best parameters:  {'classifier__max_depth': 20, 'classifier__min_samples_split': 30, 'classifier__n_estimators': 100, 'pca__n_components': 10}\n"
     ]
    }
   ],
   "source": [
    "cat_cols = X_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = X_dataset.select_dtypes(include=[\"number\"]).columns\n",
    "bool_cols = X_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "passthrough_columns = [col for col in X_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "        ])\n",
    "boolean_transformer = Pipeline([\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"bool\", boolean_transformer, bool_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"pca__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"pca\", PCA(random_state=124)),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n",
    "\n",
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the grid search with an ordinal encoder for categorical data? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    with parallel_backend('threading', n_jobs=6):\n",
    "        gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fce166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat_dataset = X_dataset.copy()\n",
    "columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"Browser Version\", \"Browser Minor\"]\n",
    "Xcat_dataset[columns] = Xcat_dataset[columns].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda76794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 660.44 seconds\n",
      "Model score: 0.332\n",
      "Best parameters:  {'classifier__max_depth': None, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100, 'pca__n_components': 6}\n"
     ]
    }
   ],
   "source": [
    "cat_cols = Xcat_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xcat_dataset.select_dtypes(include=[\"number\"]).columns\n",
    "bool_cols = Xcat_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "passthrough_columns = [col for col in Xcat_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [(\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "boolean_transformer = Pipeline([\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"bool\", boolean_transformer, bool_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"pca__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"pca\", PCA(random_state=124)),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n",
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the PCA analysis with more categorical columns? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xcat_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=6)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490004f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat_dataset = X_dataset.copy()\n",
    "columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"Browser Version\", \"Browser Minor\",\n",
    "           \"Global Source IP\", \"Global Destination IP\",\"IsWeekend\", \"Is Proxy\", \"Device Brand\", \"Device Model\", \"Browser Name\",\"OS Name\"]\n",
    "Xcat_dataset = Xcat_dataset.drop(columns=columns)\n",
    "bool_cols = Xcat_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xcat_dataset[bool_cols] = Xcat_dataset[bool_cols].astype(\"str\")\n",
    "cat_cols = Xcat_dataset.select_dtypes(include=[\"object\",\"str\"]).columns\n",
    "num_cols = Xcat_dataset.select_dtypes(include=\"number\").columns\n",
    "# Xcat_dataset[cat_cols] = Xcat_dataset[cat_cols].apply(lambda x: x.fillna(\"unknown\"))\n",
    "# numeric_transformer = Pipeline(\n",
    "#         steps = [\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "#             (\"scaler\", StandardScaler())\n",
    "#         ])\n",
    "# Xcat_dataset[num_cols] = numeric_transformer.fit_transform(Xcat_dataset[num_cols])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xcat_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c8fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough_columns = [col for col in Xcat_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ]).set_output(transform=\"pandas\")\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "    \"famd__n_components\": [2, 6, 10, 15]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"famd\", FAMD()),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a45e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florians/cours/CyberSecurity_ML/.venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 147.93 seconds\n",
      "Model score: 0.326\n",
      "Best parameters:  {'classifier__max_depth': 10, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100, 'famd__n_components': 10}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the FAMD analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    start_time = monotonic()\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=2, n_jobs=6)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f75c95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_processed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039f7e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eigenvalue</th>\n",
       "      <th>% of variance</th>\n",
       "      <th>% of variance (cumulative)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018</td>\n",
       "      <td>16.96%</td>\n",
       "      <td>16.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.010</td>\n",
       "      <td>16.83%</td>\n",
       "      <td>33.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.004</td>\n",
       "      <td>16.74%</td>\n",
       "      <td>50.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.995</td>\n",
       "      <td>16.59%</td>\n",
       "      <td>67.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>16.50%</td>\n",
       "      <td>83.62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.983</td>\n",
       "      <td>16.38%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          eigenvalue % of variance % of variance (cumulative)\n",
       "component                                                    \n",
       "0              1.018        16.96%                     16.96%\n",
       "1              1.010        16.83%                     33.79%\n",
       "2              1.004        16.74%                     50.52%\n",
       "3              0.995        16.59%                     67.12%\n",
       "4              0.990        16.50%                     83.62%\n",
       "5              0.983        16.38%                    100.00%\n",
       "6              0.000         0.00%                    100.00%\n",
       "7              0.000         0.00%                    100.00%\n",
       "8              0.000         0.00%                    100.00%\n",
       "9              0.000         0.00%                    100.00%"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "famd = FAMD(n_components=10)\n",
    "xtrain_famd = famd.fit_transform(xtrain_processed)\n",
    "famd.eigenvalues_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ae8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcat_dataset = X_dataset.copy()\n",
    "columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Second\", \"DayOfWeek\", \"Browser Version\", \"Browser Minor\"]\n",
    "Xcat_dataset[columns] = Xcat_dataset[columns].astype(\"str\")\n",
    "bool_cols = Xcat_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xcat_dataset[bool_cols] = Xcat_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9463bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xcat_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xcat_dataset.select_dtypes(include=[\"number\"]).columns\n",
    "passthrough_columns = [col for col in Xcat_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45a6009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 166.84 seconds\n",
      "Model score: 0.334\n",
      "Best parameters:  {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the HalvingGridSearchCV analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xcat_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = HalvingGridSearchCV(pipeline, param_grid, resource = \"classifier__n_estimators\", min_resources=10 , max_resources=500)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451cc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Int Source IP\", \"Int Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\", \"Packet Type\", \"Traffic Type\", \"Attack Signature\"]\n",
    "Xsim_dataset = X_dataset[columns].copy()\n",
    "bool_cols = Xsim_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xsim_dataset[bool_cols] = Xsim_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xsim_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xsim_dataset.select_dtypes(include=[\"number\"]).columns\n",
    "passthrough_columns = [col for col in Xsim_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0255be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 94.16 seconds\n",
      "Model score: 0.335\n",
      "Best parameters:  {'classifier__max_depth': 20, 'classifier__min_samples_split': 30, 'classifier__n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the simple HalvingGridSearchCV analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xsim_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = HalvingGridSearchCV(pipeline, param_grid, resource = \"classifier__n_estimators\", min_resources=10 , max_resources=1000, factor=2)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8712388",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Source Port\", \"Destination Port\", \"Protocol\", \"Packet Type\", \"Traffic Type\", \"Attack Signature\", \"Network Segment\"]\n",
    "Xsim_dataset = X_dataset[columns].copy()\n",
    "bool_cols = Xsim_dataset.select_dtypes(include=[\"bool\"]).columns\n",
    "Xsim_dataset[bool_cols] = Xsim_dataset[bool_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = Xsim_dataset.select_dtypes(include=[\"category\",\"str\"]).columns\n",
    "num_cols = Xsim_dataset.select_dtypes(include=[\"number\"]).columns\n",
    "passthrough_columns = [col for col in Xsim_dataset.columns if col not in cat_cols and col not in bool_cols and col not in num_cols]\n",
    "    \n",
    "numeric_transformer = Pipeline(\n",
    "        steps = [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_transformer, cat_cols),\n",
    "            (\"num\", numeric_transformer, num_cols)\n",
    "        ])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 10, 30],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=124))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc08f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 44.86 seconds\n",
      "Model score: 0.329\n",
      "Best parameters:  {'classifier__max_depth': None, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "relaunch =\"\"\n",
    "while relaunch.lower() not in [\"y\", \"n\"]:\n",
    "    relaunch = input(\"Do you want to relaunch the simple HalvingGridSearchCV analysis? (y/n) \")\n",
    "if relaunch.lower() == \"y\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xsim_dataset, Y_true, test_size=0.2, stratify=Y_true, random_state=124)\n",
    "    start_time = monotonic()\n",
    "    gs = HalvingGridSearchCV(pipeline, param_grid, resource = \"classifier__n_estimators\", min_resources=10 , max_resources=500)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Time taken to fit the model: %.2f seconds\" % (monotonic() - start_time))\n",
    "    print(\"Model score: %.3f\" % gs.score(X_test, y_test))\n",
    "    print(\"Best parameters: \", gs.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
